{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fd10bb4-6fa3-4a73-83ed-c513cd542605",
   "metadata": {},
   "outputs": [],
   "source": [
    "#pytorch tutorials\n",
    "#https://pytorch.org/tutorials/intermediate/char_rnn_generation_tutorial.html\n",
    "#conda env base"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5841a68d-5562-46db-a0e7-55ed5d07a261",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Slusarski\n",
      "list_files: ['Czech.txt', 'German.txt', 'Arabic.txt', 'Japanese.txt', 'Chinese.txt', 'Vietnamese.txt', 'Russian.txt', 'French.txt', 'Irish.txt', 'English.txt', 'Spanish.txt', 'Greek.txt', 'Italian.txt', 'Portuguese.txt', 'Scottish.txt', 'Dutch.txt', 'Korean.txt', 'Polish.txt']\n",
      "filenamne Czech.txt\n",
      "/Users/dc/meetup_cs224n/rnn/data/names/Czech.txt\n",
      "filenamne German.txt\n",
      "/Users/dc/meetup_cs224n/rnn/data/names/German.txt\n",
      "filenamne Arabic.txt\n",
      "/Users/dc/meetup_cs224n/rnn/data/names/Arabic.txt\n",
      "filenamne Japanese.txt\n",
      "/Users/dc/meetup_cs224n/rnn/data/names/Japanese.txt\n",
      "filenamne Chinese.txt\n",
      "/Users/dc/meetup_cs224n/rnn/data/names/Chinese.txt\n",
      "filenamne Vietnamese.txt\n",
      "/Users/dc/meetup_cs224n/rnn/data/names/Vietnamese.txt\n",
      "filenamne Russian.txt\n",
      "/Users/dc/meetup_cs224n/rnn/data/names/Russian.txt\n",
      "filenamne French.txt\n",
      "/Users/dc/meetup_cs224n/rnn/data/names/French.txt\n",
      "filenamne Irish.txt\n",
      "/Users/dc/meetup_cs224n/rnn/data/names/Irish.txt\n",
      "filenamne English.txt\n",
      "/Users/dc/meetup_cs224n/rnn/data/names/English.txt\n",
      "filenamne Spanish.txt\n",
      "/Users/dc/meetup_cs224n/rnn/data/names/Spanish.txt\n",
      "filenamne Greek.txt\n",
      "/Users/dc/meetup_cs224n/rnn/data/names/Greek.txt\n",
      "filenamne Italian.txt\n",
      "/Users/dc/meetup_cs224n/rnn/data/names/Italian.txt\n",
      "filenamne Portuguese.txt\n",
      "/Users/dc/meetup_cs224n/rnn/data/names/Portuguese.txt\n",
      "filenamne Scottish.txt\n",
      "/Users/dc/meetup_cs224n/rnn/data/names/Scottish.txt\n",
      "filenamne Dutch.txt\n",
      "/Users/dc/meetup_cs224n/rnn/data/names/Dutch.txt\n",
      "filenamne Korean.txt\n",
      "/Users/dc/meetup_cs224n/rnn/data/names/Korean.txt\n",
      "filenamne Polish.txt\n",
      "/Users/dc/meetup_cs224n/rnn/data/names/Polish.txt\n",
      "18\n",
      "['Nguyen', 'Tron', 'Le', 'Pham', 'Huynh', 'Hoang', 'Phan', 'Vu', 'Vo', 'Dang', 'Bui', 'Do', 'Ho', 'Ngo', 'Duong', 'Ly', 'An', 'an', 'Bach', 'Banh', 'Cao', 'Chau', 'Chu', 'Chung', 'Chu', 'Diep', 'Doan', 'Dam', 'Dao', 'Dinh', 'Doan', 'Giang', 'Ha', 'Han', 'Kieu', 'Kim', 'La', 'Lac', 'Lam', 'Lieu', 'Luc', 'Luong', 'Luu', 'Ma', 'Mach', 'Mai', 'Nghiem', 'Phi', 'Pho', 'Phung', 'Quach', 'Quang', 'Quyen', 'Ta', 'Thach', 'Thai', 'Sai', 'Thi', 'Than', 'Thao', 'Thuy', 'Tieu', 'To', 'Ton', 'Tong', 'Trang', 'Trieu', 'Trinh', 'Truong', 'Van', 'Vinh', 'Vuong', 'Vuu']\n",
      "['Nguyen', 'Tron', 'Le', 'Pham', 'Huynh', 'Hoang', 'Phan', 'Vu', 'Vo', 'Dang', 'Bui', 'Do', 'Ho', 'Ngo', 'Duong', 'Ly', 'An', 'an', 'Bach', 'Banh', 'Cao', 'Chau', 'Chu', 'Chung', 'Chu', 'Diep', 'Doan', 'Dam', 'Dao', 'Dinh', 'Doan', 'Giang', 'Ha', 'Han', 'Kieu', 'Kim', 'La', 'Lac', 'Lam', 'Lieu', 'Luc', 'Luong', 'Luu', 'Ma', 'Mach', 'Mai', 'Nghiem', 'Phi', 'Pho', 'Phung', 'Quach', 'Quang', 'Quyen', 'Ta', 'Thach', 'Thai', 'Sai', 'Thi', 'Than', 'Thao', 'Thuy', 'Tieu', 'To', 'Ton', 'Tong', 'Trang', 'Trieu', 'Trinh', 'Truong', 'Van', 'Vinh', 'Vuong', 'Vuu']\n"
     ]
    }
   ],
   "source": [
    "#https://pytorch.org/tutorials/intermediate/char_rnn_classification_tutorial\n",
    "import torch\n",
    "import os\n",
    "import unicodedata2\n",
    "import string\n",
    "\n",
    "list_files = os.listdir(os.getcwd()+'/data/names')\n",
    "all_letters = string.ascii_letters + \" .,;'\" #is the punc necessary?\n",
    "\n",
    "\n",
    "def unicodeToAscii(s):\n",
    "    return ''.join(\n",
    "        c for c in unicodedata2.normalize('NFD', s)\n",
    "        if unicodedata2.category(c) != 'Mn'\n",
    "        and c in all_letters\n",
    "    )\n",
    "print(unicodeToAscii('Ślusàrski'))\n",
    "\n",
    "def read_lines(filename):\n",
    "    print(\"filenamne\",filename)\n",
    "    print(os.getcwd()+'/data/names/'+filename)\n",
    "    lines = open(os.getcwd()+'/data/names/'+filename).read().strip().split('\\n')\n",
    "    return lines\n",
    "\n",
    "    \n",
    "#for filename in list_files:\n",
    "#    stuff = read_lines(filename)\n",
    "print(\"list_files:\",list_files)\n",
    "category_lines = [read_lines(filename) for filename in list_files if filename.endswith('.txt')]\n",
    "print(len(category_lines)) #18\n",
    "#spot check 5th Vietnamese.txt\n",
    "print(category_lines[5])\n",
    "category_dict = dict(zip(list_files, category_lines))\n",
    "print(category_dict['Vietnamese.txt'])\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "579cd61a-0d7c-457c-a0f3-137acd3b3247",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "name: Nguyen\n",
      "name_to_tensor Nguyen: [tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0.]), tensor([0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0.]), tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0.]), tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0.]), tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0.]), tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0.])]\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "----------\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n"
     ]
    }
   ],
   "source": [
    "# onehot\n",
    "def letter_to_tensor(letter):\n",
    "    #print(\"letter:\",letter)\n",
    "    idx = all_letters.find(letter)\n",
    "    tnsr = torch.zeros(len(all_letters))\n",
    "    tnsr[idx] = 1\n",
    "    return tnsr\n",
    "\n",
    "def name_to_tensor(name):\n",
    "    print(\"name:\",name)\n",
    "    return [onehot_letter(letter) for letter in name]\n",
    "\n",
    "#test = onehot('j')\n",
    "#print(test)\n",
    "print(\"name_to_tensor Nguyen:\",name_to_tensor('Nguyen'))\n",
    "\n",
    "a=[1,2,3,4]\n",
    "print(*a, sep='\\n')\n",
    "\n",
    "print(\"-\"*10)\n",
    "#this is best because it doesnt require the list comprehension\n",
    "#map applies fn str to each element. this looks more like a data pipeline\n",
    "print('\\n'.join(map(str,a)))\n",
    "\n",
    "\n",
    "def onehot_lines(lines):\n",
    "    names = [x for x in lines]\n",
    "    print(\"names:\",names)\n",
    "    on = [name_to_tensor(name) for name in names]\n",
    "    print(\"on:\", on)\n",
    "    #print(\"onehot_lines:\",onehot_lines)\n",
    "#print(onehot_lines(category_dict['Vietnamese.txt']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bb9b004-8405-4050-8410-4b5cc7f53a0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def letter_tensor(letter):\n",
    "    \"\"\"\"\"\"\n",
    "    \n",
    "def name_tensor(name):\n",
    "    \"\"\"\"\"\"\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa15e439-d047-451f-9fcf-eb33afdf2621",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNN(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        super(RNN,self).__init__(self)\n",
    "        \n",
    "        self.hidden_size = hidden_size\n",
    "        \n",
    "        self.input = nn.Linear(input_size, self.hidden_size)\n",
    "        self.hidden = nn.Linear(self.hidden_size, self.hidden_size)\n",
    "        self.output = nn.Linear(self.hidden_size, output_size)\n",
    "        self.softmax  = nn.LogSoftmax(dim=1)\n",
    "    \n",
    "    def forward(self,input, hidden):\n",
    "        hidden = F.tanh(self.input(input+self.hidden(hidden)))\n",
    "        output = self.output(hidden)\n",
    "        output = self.softmax(output)\n",
    "        return output, hidden\n",
    "    \n",
    "    def initHidden(self):\n",
    "        return torch.zeros(1, self.hidden_size)\n",
    "n_hidden=128\n",
    "rnn = RNN(n_letters, n_hidden, n_categories)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "c9d665f3-5fa5-45ce-a668-d79e324a861e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['data/names/Czech.txt', 'data/names/German.txt', 'data/names/Arabic.txt', 'data/names/Japanese.txt', 'data/names/Chinese.txt', 'data/names/Vietnamese.txt', 'data/names/Russian.txt', 'data/names/French.txt', 'data/names/Irish.txt', 'data/names/English.txt', 'data/names/Spanish.txt', 'data/names/Greek.txt', 'data/names/Italian.txt', 'data/names/Portuguese.txt', 'data/names/Scottish.txt', 'data/names/Dutch.txt', 'data/names/Korean.txt', 'data/names/Polish.txt']\n",
      "n_letters: 57\n",
      "Slusarski\n"
     ]
    }
   ],
   "source": [
    "from io import open\n",
    "import glob\n",
    "import os\n",
    "\n",
    "def findFiles(path): return glob.glob(path)\n",
    "\n",
    "print(findFiles('data/names/*.txt'))\n",
    "\n",
    "import unicodedata\n",
    "import string\n",
    "\n",
    "all_letters = string.ascii_letters + \" .,;'\"\n",
    "n_letters = len(all_letters)\n",
    "print(\"n_letters:\",n_letters)\n",
    "\n",
    "# Turn a Unicode string to plain ASCII, thanks to https://stackoverflow.com/a/518232/2809427\n",
    "def unicodeToAscii(s):\n",
    "    return ''.join(\n",
    "        c for c in unicodedata.normalize('NFD', s)\n",
    "        if unicodedata.category(c) != 'Mn'\n",
    "        and c in all_letters\n",
    "    )\n",
    "\n",
    "print(unicodeToAscii('Ślusàrski'))\n",
    "\n",
    "# Build the category_lines dictionary, a list of names per language\n",
    "category_lines = {}\n",
    "all_categories = []\n",
    "\n",
    "# Read a file and split into lines\n",
    "def readLines(filename):\n",
    "    lines = open(filename, encoding='utf-8').read().strip().split('\\n')\n",
    "    return [unicodeToAscii(line) for line in lines]\n",
    "\n",
    "for filename in findFiles('data/names/*.txt'):\n",
    "    category = os.path.splitext(os.path.basename(filename))[0]\n",
    "    all_categories.append(category)\n",
    "    lines = readLines(filename)\n",
    "    category_lines[category] = lines\n",
    "\n",
    "n_categories = len(all_categories)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "07c9b19f-f8a0-4394-a161-1db6e8c94924",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Abandonato', 'Abatangelo', 'Abatantuono', 'Abate', 'Abategiovanni']\n"
     ]
    }
   ],
   "source": [
    "print(category_lines['Italian'][:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "b0a1cdea-6a02-47eb-94c3-894c932e9980",
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "index 1 is out of bounds for dimension 0 with size 1",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[27], line 23\u001b[0m\n\u001b[1;32m     19\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m tensor\n\u001b[1;32m     21\u001b[0m \u001b[38;5;66;03m#print(letterToTensor('J'))\u001b[39;00m\n\u001b[0;32m---> 23\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[43mlineToTensor\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mNguyen\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39msize())\n\u001b[1;32m     24\u001b[0m \u001b[38;5;28mprint\u001b[39m(lineToTensor(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mNguyen\u001b[39m\u001b[38;5;124m'\u001b[39m))\n",
      "Cell \u001b[0;32mIn[27], line 18\u001b[0m, in \u001b[0;36mlineToTensor\u001b[0;34m(line)\u001b[0m\n\u001b[1;32m     16\u001b[0m tensor \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mzeros(\u001b[38;5;28mlen\u001b[39m(line), \u001b[38;5;241m1\u001b[39m, n_letters)\n\u001b[1;32m     17\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m li, letter \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(line):\n\u001b[0;32m---> 18\u001b[0m     \u001b[43mtensor\u001b[49m\u001b[43m[\u001b[49m\u001b[43mli\u001b[49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m[letterToIndex(letter)] \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m     19\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m tensor\n",
      "\u001b[0;31mIndexError\u001b[0m: index 1 is out of bounds for dimension 0 with size 1"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# Find letter index from all_letters, e.g. \"a\" = 0\n",
    "def letterToIndex(letter):\n",
    "    return all_letters.find(letter)\n",
    "\n",
    "# Just for demonstration, turn a letter into a <1 x n_letters> Tensor\n",
    "def letterToTensor(letter):\n",
    "    tensor = torch.zeros(1, n_letters)\n",
    "    tensor[0][letterToIndex(letter)] = 1\n",
    "    return tensor\n",
    "\n",
    "# Turn a line into a <line_length x 1 x n_letters>,\n",
    "# or an array of one-hot letter vectors\n",
    "def lineToTensor(line):\n",
    "    tensor = torch.zeros(len(line), 1, n_letters)\n",
    "    for li, letter in enumerate(line):\n",
    "        tensor[li][0][letterToIndex(letter)] = 1\n",
    "    return tensor\n",
    "\n",
    "#print(letterToTensor('J'))\n",
    "\n",
    "print(lineToTensor('Nguyen').size())\n",
    "print(lineToTensor('Nguyen'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "1f9b5111-67c8-4889-905f-f314e79744bb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([10])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.zeros(10).size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "5ce29056-a562-4608-9f3f-27d8ed3b014e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[0., 0., 0., 0., 0.]],\n",
       "\n",
       "        [[0., 0., 0., 0., 0.]],\n",
       "\n",
       "        [[0., 0., 0., 0., 0.]]])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.zeros(3,1,5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "e64b2a35-8c51-4d19-8ac0-95ffdd74fdc4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[0., 0., 0., 0., 0.]],\n",
       "\n",
       "        [[0., 0., 0., 0., 0.]],\n",
       "\n",
       "        [[0., 0., 0., 0., 0.]]])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.zeros(3,1,5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d0469d0-d662-4b6e-b5b6-81a52622cbec",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

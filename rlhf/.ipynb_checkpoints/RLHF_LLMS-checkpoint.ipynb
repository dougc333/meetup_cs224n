{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5d4e491b-bba9-429d-9378-f4badc6f2f87",
   "metadata": {},
   "source": [
    "<h4>RNNs,LSTMs, GRU, MultiLayer RNNs, RNNs + Transformer </h4>\n",
    "<h4>Self attention and multihead attention</h4>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b8a0fbb-e133-4d5d-8287-c238148f8e9b",
   "metadata": {},
   "source": [
    "<h4>Fine Tuning LLMS</h4>\n",
    "<h4>Reinforcement Learning with Human Feedback</h4>\n",
    "<p>ChatGPT does not use the log loss functions we are coding up. They supplement it with RL and other proprietary methods </p>\n",
    "<img src=\"rlhf.png\"/ >"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3472fe1-1b7a-4e2b-b5d0-9b1ee57e23b0",
   "metadata": {},
   "source": [
    "</h4>RLHF Adjustments for Tractability</h4>\n",
    "<h4>RL Proximal Policy Optimization Original Paper w code</h4>\n",
    "<a href=\"https://arxiv.org/pdf/1707.06347\">paper reference</a>\n",
    "<h4>Gradient Policy Methods</h4>\n",
    "<h5>Gradient Estimator</h5>\n",
    "$$\\hat{g} = \\hat{E_t}[\\nabla_{\\theta} logP_{\\theta}(\\alpha_t | s_t)\\hat{A_t}] $$\n",
    "<h5>Estimating policy gradient Loss Fn: </h5>\n",
    "$$L^{PG}(\\theta) = \\hat{E_t}[logP_{\\theta}(\\alpha_t | s_{t})\\hat{A_t}]$$\n",
    "<h5>Intractable bc large policy updates, add modification via TRPO</h5>\n",
    "$$max_{\\theta} E_{\\theta}[\\frac{P_{\\theta}(\\alpha_t | s_t)}{P_{old}(\\alpha_t | s_t)}\\hat{A_t}]$$\n",
    "\n",
    "<h5>LCLIP</h5>\n",
    "$$L^{CLIP}(\\theta) = \\hat{E_t}[min(r_t(\\theta)\\hat{A_t},clip(r_t{\\theta}, 1-\\epsilon, 1+epsilon)\\hat{A_t})]$$\n",
    "<a href=\"https://github.com/openai/baselines/tree/master/baselines\">RL PPO</a>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "316ea147-6354-4cfd-a299-acb93447ed72",
   "metadata": {},
   "source": [
    "<h4>RLHF with DPO</h4>\n",
    "<h5>DeepLearningAI short course on RLHF, no code</h5>\n",
    "<a href=\"https://learn.deeplearning.ai/courses/reinforcement-learning-from-human-feedback/lesson/2/how-does-rlhf-work\"></a>\n",
    "\n",
    "<img src=\"rlhf_gc1.png\" />\n",
    "<h4>There are 2 models being trained, a reward model and RL policy. </h4>\n",
    "<img src=\"rlhf_gc2.png\" />\n",
    "<img src=\"rlhf_gc3.png\" />\n",
    "<img src=\"rlhf_gc4.png\" />\n",
    "<img src=\"rlhf_gc5.png\" />\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12fad9af-80d9-4f9f-af20-6e40584a5bb9",
   "metadata": {},
   "source": [
    "<h4>Runs on GCP Vertex AI pipeline</h4>\n",
    "<ol>\n",
    "<li>GCP source code not easy to access. Is it published? </li>\n",
    "<li>Dataset link: https://huggingface.co/datasets/webis/tldr-17</li>\n",
    "</ol>\n",
    "<h4>1 day to run, 24h * 4$/hr? maybe 100$ to run it; if it works</h4>\n",
    "<h4>Additional Resources/h4>\n",
    "<ol>\n",
    "<li> RL TRPO/PPO lecture https://www.youtube.com/watch?v=H8rElrvs9Lo</li>\n",
    "<li>HW link: https://www.youtube.com/watch?v=4RyX7L-MbsU</li>\n",
    "<li>https://github.com/upb-lea/reinforcement_learning_course_materials </li> \n",
    "</ol>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8d3646b-0624-4b7b-92d0-76982e43fe92",
   "metadata": {},
   "source": [
    "<h4>Many Steps in RLHF, eg SFT</h4>\n",
    "<img src=\"rlhf_block.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebeababa-c8f5-4299-a85e-b3fb077313de",
   "metadata": {},
   "source": [
    "<h4>Hugging Face RLHF pipeline = SFT+Reward Modeling+PPO step </h4>\n",
    "<h4>SFT</h4>\n",
    "<code>\n",
    "from datasets import load_dataset\n",
    "from trl import SFTConfig, SFTTrainer\n",
    "\n",
    "dataset = load_dataset(\"imdb\", split=\"train\")\n",
    "\n",
    "sft_config = SFTConfig(\n",
    "    dataset_text_field=\"text\",\n",
    "    max_seq_length=512,\n",
    "    output_dir=\"/tmp\",\n",
    ")\n",
    "trainer = SFTTrainer(\n",
    "    \"facebook/opt-350m\",\n",
    "    train_dataset=dataset,\n",
    "    args=sft_config,\n",
    ")\n",
    "trainer.train()\n",
    "</code>\n",
    "<h4>Reward Modeling</h4>\n",
    "<code>\n",
    "from peft import LoraConfig, TaskType\n",
    "from transformers import AutoModelForSequenceClassification, AutoTokenizer\n",
    "from trl import RewardTrainer, RewardConfig\n",
    "\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\"gpt2\")\n",
    "peft_config = LoraConfig(\n",
    "    task_type=TaskType.SEQ_CLS,\n",
    "    inference_mode=False,\n",
    "    r=8,\n",
    "    lora_alpha=32,\n",
    "    lora_dropout=0.1,\n",
    ")\n",
    "\n",
    "...\n",
    "\n",
    "trainer = RewardTrainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    tokenizer=tokenizer,\n",
    "    train_dataset=dataset,\n",
    "    peft_config=peft_config,\n",
    ")\n",
    "\n",
    "trainer.train()\n",
    "\n",
    "</code>\n",
    "<h4>PPO Step</h4>\n",
    "<code>\n",
    "python -i examples/scripts/ppo/ppo.py \\\n",
    "    --learning_rate 3e-6 \\\n",
    "    --num_ppo_epochs 1 \\\n",
    "    --num_mini_batches 1 \\\n",
    "    --output_dir models/minimal/ppo \\\n",
    "    --per_device_train_batch_size 64 \\\n",
    "    --gradient_accumulation_steps 1 \\\n",
    "    --total_episodes 10000 \\\n",
    "    --model_name_or_path EleutherAI/pythia-1b-deduped \\\n",
    "    --non_eos_penalty \\\n",
    "</code>\n",
    "<img src=\"ppo_metrics.png\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac1a9668-386d-4fab-a801-2155d00131f0",
   "metadata": {},
   "source": [
    "<h4>RL with DPO</h4>\n",
    "<p>Paper Link: https://arxiv.org/abs/2305.18290</p>\n",
    "<h4>RLHF with DPO:</h4>\n",
    "<p>Code from HF: https://github.com/huggingface/blog/blob/main/pref-tuning.md</p>\n",
    "<ol>\n",
    "<li>There are 2 loss functions, a reward loss fn and a policy reward fn. </li>\n",
    "<li>Soruce: https://web.stanford.edu/class/cs234/slides/dpo_slides.pdf</li>\n",
    "</ol>\n",
    "<img src=\"rl_dpo1.png\" />\n",
    "<img src=\"rl_dpo2.png\" />\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "baf40694-9f89-4e02-b22f-a7be2af0ff9d",
   "metadata": {},
   "source": [
    "<h4>HF DPO Trainer</h4>\n",
    "<code>\n",
    "import torch\n",
    "from trl import DPOConfig, DPOTrainer\n",
    "from unsloth import FastLanguageModel\n",
    "\n",
    "max_seq_length = 2048 # Supports automatic RoPE Scaling, so choose any number.\n",
    "\n",
    "# Load model\n",
    "model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "    model_name = \"unsloth/zephyr-sft\",\n",
    "    max_seq_length = max_seq_length,\n",
    "    dtype = None, # None for auto detection. Float16 for Tesla T4, V100, Bfloat16 for Ampere+\n",
    "    load_in_4bit = True, # Use 4bit quantization to reduce memory usage. Can be False.\n",
    "    # token = \"hf_...\", # use one if using gated models like meta-llama/Llama-2-7b-hf\n",
    ")\n",
    "\n",
    "# Do model patching and add fast LoRA weights\n",
    "model = FastLanguageModel.get_peft_model(\n",
    "    model,\n",
    "    r = 16,\n",
    "    target_modules = [\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n",
    "                      \"gate_proj\", \"up_proj\", \"down_proj\",],\n",
    "    lora_alpha = 16,\n",
    "    lora_dropout = 0, # Dropout = 0 is currently optimized\n",
    "    bias = \"none\",    # Bias = \"none\" is currently optimized\n",
    "    use_gradient_checkpointing = True,\n",
    "    random_state = 3407,\n",
    ")\n",
    "\n",
    "training_args = DPOConfig(\n",
    "    output_dir=\"./output\",\n",
    "    beta=0.1,\n",
    ")\n",
    "\n",
    "dpo_trainer = DPOTrainer(\n",
    "    model,\n",
    "    ref_model=None,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    tokenizer=tokenizer,\n",
    ")\n",
    "dpo_trainer.train()\n",
    "</code>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61c846c8-14d7-4f3e-954f-6603325300db",
   "metadata": {},
   "outputs": [],
   "source": [
    "<h4>Pytorch Kernels</h4>\n",
    "<ol>\n",
    "<li>CPU or GPU kernels different than using nn.Module. Parallelism not specified in nn.Module</li>\n",
    "<li>A kernel manages the parallelism in a code block. CPU cores vs. thousands of GPU threads</li>\n",
    "<li>Optimize for training means GPU Kernels</li>\n",
    "<li>Strategy 1: NVIDIA cuBLAS using hand optimized CUDA code</li>\n",
    "<li>Strategy 2: NVIDIA Cutlass: https://github.com/NVIDIA/cutlass</li>\n",
    "<li>Strategy 3: OpenAI/Pytorch Triton</li>\n",
    "</ol>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcde1b5e-0f5a-49de-b209-78a848b0a5d5",
   "metadata": {},
   "source": [
    "<h4>Simple approach to optimization</h4>\n",
    "<p> Add torch compile<p>\n",
    "<code>\n",
    "    def train(mod, data):\n",
    "        opt.zero_grad(True)\n",
    "        pred = mod(data[0])\n",
    "        loss = torch.nn.CrossEntropyLoss()(pred, data[1])\n",
    "        loss.backward()\n",
    "        opt.step()\n",
    "\n",
    "    eager_times = []\n",
    "    for i in range(N_ITERS):\n",
    "        inp = generate_data(16)\n",
    "        _, eager_time = timed(lambda: train(model, inp))\n",
    "        eager_times.append(eager_time)\n",
    "        print(f\"eager train time {i}: {eager_time}\")\n",
    "    print(\"~\" * 10)\n",
    "\n",
    "    model = init_model()\n",
    "    opt = torch.optim.Adam(model.parameters())\n",
    "    train_opt = torch.compile(train, mode=\"reduce-overhead\")\n",
    "</code>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f9a41c6-8073-4fd4-970c-9827e1840179",
   "metadata": {},
   "source": [
    "<h4>RLHF accuracy of 67 </h4>\n",
    "https://huggingface.co/blog/stackllama\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65fca8b8-88cd-4ec9-ae93-f3a08c583f65",
   "metadata": {},
   "source": [
    "<h4>Raschka LLM </h4>\n",
    "<p>\n",
    "Links: https://sebastianraschka.com/blog/2024/research-papers-in-march-2024.html\n",
    "</p>\n",
    "<p>\n",
    "Links: https://magazine.sebastianraschka.com/p/llm-training-rlhf-and-its-alternatives\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "071a4a37-8d05-472f-a777-d23709a6c302",
   "metadata": {},
   "source": [
    "<h4>Triton Compiler and Language</h4>\n",
    "<ol>\n",
    "<li>Triton language looks like assembly (load/store)</li>\n",
    "<li>OpenAI/FB/Google have similar tools to recode from python to device native formats for efficiency. Est 2-4x speedup. Prevent data starvation for multiple jobs</li>\n",
    "<li>Open sourced to try to get more contributions</li>   \n",
    "<li>Inference requires triton</li>\n",
    "<li>https://www.youtube.com/watch?v=Nh5QIkGuExQ</li>\n",
    "</ol>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f426053a-f249-40d5-8ae7-ec291a5ea285",
   "metadata": {},
   "outputs": [],
   "source": [
    "<h4>Pytorch Cutlass</h4>\n",
    "<p>https://www.youtube.com/watch?v=yCyZEJrlrfY</p>\n",
    "Python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "286ea4e4-ec0b-49d7-a141-798d431d71ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "<h4>FB tools</h4>\n",
    "<h4>FB AI Template for inference serving.</h4> \n",
    "https://github.com/facebookincubator/AITemplate\n",
    "<h4>Transformer templates from FB</h4>\n",
    "https://github.com/facebookresearch/xformers\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
